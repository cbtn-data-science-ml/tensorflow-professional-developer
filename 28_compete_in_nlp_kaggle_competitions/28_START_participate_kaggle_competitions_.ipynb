{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP2qbxQjibNl"
   },
   "source": [
    "# Sequence Models: Kaggle Competitions\n",
    "This notebook uses `TextVectorization` instead of `Tokenizer` for text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rpMXyJBibNn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "!git clone https://github.com/cbtn-data-science-ml/tensorflow-professional-developer.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY2pqEPFjZNr"
   },
   "outputs": [],
   "source": [
    "# print working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LGD1hIvjZ6y"
   },
   "outputs": [],
   "source": [
    "# change into project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXHqvYCe7lTK"
   },
   "outputs": [],
   "source": [
    "# list files and ensure you see nlp_disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOCJazH1jec9"
   },
   "outputs": [],
   "source": [
    "# convert train.csv and test.csv into DataFrame objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOMXu8xSi3_9"
   },
   "outputs": [],
   "source": [
    "# word count for each tweet\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t2vU1G97-t5"
   },
   "outputs": [],
   "source": [
    "# visualize 'Tweet Word Count Distribution'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['word_count'], bins=30, kde=True)\n",
    "plt.title('Tweet Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqnpwV7ri39X"
   },
   "outputs": [],
   "source": [
    "# use .info() on train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQN1Fawfi36x"
   },
   "outputs": [],
   "source": [
    "# run cell to sum tweets that contain www, http, and https\n",
    "urls_before_cleaning = train_df['text'].str.contains(r'http\\S+|www\\S+|https\\S+').sum()\n",
    "print(f\"URLs found before cleaning: {urls_before_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IroX2HDvjFTi"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk # natural language toolkit (if needed, most for Jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEzU1Vx3i339"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# build function to clean our text data\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'<.*?>', '', text)\n",
    "  text = re.sub(r'[^a-z\\s]', '', text)\n",
    "  tokens = word_tokenize(text)\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "  return \" \".join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuLe6UNJi307"
   },
   "outputs": [],
   "source": [
    "# Apply the cleaning function to train_df['text'] and test_df['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfUVaKVo86eS"
   },
   "outputs": [],
   "source": [
    "# run cell to sum tweets that contain www, http, and https\n",
    "urls_before_cleaning = train_df['clean_text'].str.contains(r'http\\S+|www\\S+|https\\S+').sum()\n",
    "print(f\"URLs found after cleaning: {urls_before_cleaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdYbmN9ii3rI"
   },
   "outputs": [],
   "source": [
    "# create Hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6MTda8rjLw9"
   },
   "outputs": [],
   "source": [
    "# create updated TextVectorization Layer (Tokenizer() is deprecated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps6FkJi59WB7"
   },
   "outputs": [],
   "source": [
    "# run cell to preprocess the data: Vectorize the text and convert X to NumPy array (needed for TextVectorization: it expects this format)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o08RgL3Dlq0x"
   },
   "outputs": [],
   "source": [
    "# run cell to import model_utils.py and relevant functions\n",
    "!wget https://raw.githubusercontent.com/cbtn-data-science-ml/introduction-to-deep-learning/main/model_utils.py\n",
    "from model_utils import early_stopping_callback, model_checkpoint_callback, plot_loss_and_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QenGtjlljOcT"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "\n",
    "# Compile model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWGkyo-wnTGM"
   },
   "outputs": [],
   "source": [
    "# plot_loss_and_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeT-1hQg9wk0"
   },
   "source": [
    "# Competition Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG-X5uHUfEIG"
   },
   "outputs": [],
   "source": [
    "# Prepare Kaggle Contest Output\n",
    "\n",
    "# Vectorize the test dataset\n",
    "X_test = vectorizer(test_df['clean_text'].astype(str)).numpy()\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Create a submission file for Kaggle\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'target': predictions_binary})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0BZ4zfj901m"
   },
   "source": [
    "# Bonus: Embedding Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Iz3vADnmRh7"
   },
   "outputs": [],
   "source": [
    "# Link: https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n",
    "# Create a reverse index for words from the TextVectorization vocabulary\n",
    "def get_reverse_index(vectorizer):\n",
    "    vocabulary = vectorizer.get_vocabulary()\n",
    "    return {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "reverse_index = get_reverse_index(vectorizer)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'model' is your trained model and the first layer is the embedding layer\n",
    "embedding_layer = model.layers[0]  # The Embedding layer is the first layer in the Sequential model\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Save the embeddings for TensorFlow Embedding Projector\n",
    "np.savetxt(\"vectors.tsv\", embeddings, delimiter=\"\\t\")\n",
    "\n",
    "# Prepare the metadata file\n",
    "with open(\"metadata.tsv\", \"w\", encoding='utf-8') as f:\n",
    "    # Explicitly account for the padding token\n",
    "    f.write(\"<PAD>\\n\")\n",
    "    # Write the words from the reverse index\n",
    "    for i in range(1, len(reverse_index)):  # Skip the padding token\n",
    "        f.write(reverse_index[i] + \"\\n\")\n",
    "\n",
    "# Download the files for TensorFlow Embedding Projector\n",
    "from google.colab import files\n",
    "files.download(\"vectors.tsv\")\n",
    "files.download(\"metadata.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE0AA8fM92Fi"
   },
   "source": [
    "# Challenge\n",
    "Build a Functional API version to improve model performance. You can use this to check your score on Kaggle competition to get an idea but as per the rules, do not sumbit to the disaster tweets competition. Instead, see these to list a few:\n",
    "\n",
    "\n",
    "*   https://www.kaggle.com/code/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model\n",
    "*   https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAR3ZStNmzUh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "\n",
    "# Define the input layer\n",
    "\n",
    "\n",
    "# Add embedding layer\n",
    "\n",
    "\n",
    "# Add bidirectional LSTM layers\n",
    "\n",
    "\n",
    "# Add dropout for regularization\n",
    "\n",
    "# Add dense output layer\n",
    "\n",
    "\n",
    "# Create the model\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLa-qy2VnT2V"
   },
   "outputs": [],
   "source": [
    "plot_loss_and_accuracy()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
