{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfyIBBOnNGhp"
   },
   "source": [
    "# Review:\n",
    "## Single-Layer LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "1a40e6f6",
    "outputId": "b3594a94-46b9-4992-d885-3e76c6f0c88b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Single_LSTM_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Single_LSTM_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define model parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64\n",
    "MAX_LEN = 25\n",
    "LSTM_UNITS = 16\n",
    "\n",
    "# Build single-layer LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=EMBEDDING_OUTPUT_DIM),\n",
    "    LSTM(LSTM_UNITS),\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name=\"Single_LSTM_Model\")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJZFZBn1xha9"
   },
   "source": [
    "## Bidirectional LSTM\n",
    "This example demonstrates setting up a bidirectional LSTM layer using TensorFlow and Keras, processing inputs in both forward and reverse directions and combining the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbiAKv4mxk7W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64  # Embedding dimensions\n",
    "MAX_LEN = 25  # Maximum length of input sequence\n",
    "LSTM_UNITS = 16  # Number of units in each LSTM direction\n",
    "VOCAB_SIZE = 1000  # Size of vocabulary (e.g., 1000 unique words)\n",
    "\n",
    "# Define a bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM),\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),  # Bidirectional LSTM layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY046bfFxxPU"
   },
   "source": [
    "## Single-Layer GRU\n",
    "This example demonstrates setting up a single-layer GRU in TensorFlow and Keras, which processes the sequence data in the forward direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "u3cBjFZux5Si",
    "outputId": "5e5dd853-a44c-43f6-db15-acf68e495d18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64  # Embedding dimensions\n",
    "MAX_LEN = 25  # Maximum length of input sequence\n",
    "GRU_UNITS = 16  # Number of units in the GRU layer\n",
    "VOCAB_SIZE = 1000  # Size of vocabulary (e.g., 1000 unique words)\n",
    "\n",
    "# Define a single-layer GRU model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM, input_length=MAX_LEN),\n",
    "    GRU(GRU_UNITS, return_sequences=False),  # Single GRU layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2EGpbsA6VXV"
   },
   "source": [
    "## Bidirectional GRU\n",
    "Here we set up a bidirectional GRU layer, which also processes each input in both directions, combining the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "MaToI03F6d6R",
    "outputId": "77bf5d97-ec65-4a19-ee4e-02ce72563d02"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64  # Embedding dimensions\n",
    "MAX_LEN = 25  # Maximum length of input sequence\n",
    "GRU_UNITS = 16  # Number of units in each GRU direction\n",
    "VOCAB_SIZE = 1000  # Size of vocabulary (e.g., 1000 unique words)\n",
    "\n",
    "# Define a bidirectional GRU model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(GRU(GRU_UNITS, return_sequences=True)),  # Bidirectional GRU layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv426CS1yyQ1"
   },
   "source": [
    "# Challenge\n",
    "Use one of the above model architechtures and also consider using a functional API approach. Here is an example to review:\n",
    "\n",
    "### Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTDJtMQN7Ojy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64  # Embedding dimensions\n",
    "MAX_LEN = 25  # Maximum length of input sequence\n",
    "RNN_UNITS = 16  # Number of units in each RNN direction\n",
    "VOCAB_SIZE = 1000  # Size of vocabulary (e.g., 1000 unique words)\n",
    "\n",
    "# Define a simple RNN model using the Sequential API\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM, input_length=MAX_LEN),\n",
    "    SimpleRNN(RNN_UNITS, return_sequences=True),  # Simple RNN layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THHO4eqt7WaY"
   },
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i5Y3Nj67ZWe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64  # Embedding dimensions\n",
    "MAX_LEN = 25  # Maximum length of input sequence\n",
    "RNN_UNITS = 16  # Number of units in each RNN direction\n",
    "VOCAB_SIZE = 1000  # Size of vocabulary (e.g., 1000 unique words)\n",
    "\n",
    "# Define a simple RNN model using the Functional API\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM)(inputs)\n",
    "x = SimpleRNN(RNN_UNITS, return_sequences=True)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"Functional_RNN_Model\")\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd-YykQl09Ri"
   },
   "source": [
    "# DNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usuk9HKfjgHy"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "# Part 1: Download and Load the Corpus: https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "\n",
    "\n",
    "# Define corpus\n",
    "corpus =  # Limit the corpus size to 10,000 characters\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "\n",
    "# Define TextVectorization layer\n",
    "\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "\n",
    "\n",
    "# Generate n-grams from the numerical representation\n",
    "\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "\n",
    "\n",
    "# Create X and y from the Padded Sequences\n",
    "\n",
    "\n",
    "# Part 3: Define and Compile the Model using Sequential API\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZOMpNfa05k5"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR3qE1O0ONu1"
   },
   "source": [
    "# SimpleRNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TatdqcIi0qc"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "# Use a larger subset of the corpus to improve training\n",
    "corpus = corpus[:50000]  # Limit the corpus size to 50,000 characters (then 50k)\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams from the numerical representation\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create X and y from the Padded Sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "# Part 3: Define and Compile the Model using Sequential API\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8Jcsj0n07xo"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtA20m8d7bZD"
   },
   "source": [
    "# LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXT6adIy0Qk"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "# Use a larger subset of the corpus to improve training\n",
    "corpus = corpus[:50000]  # Limit the corpus size to 50,000 characters (then 50k)\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams from the numerical representation\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create X and y from the Padded Sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "# Part 3: Define and Compile the Model using Sequential API\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9Y5LlUVU_cS"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLujd6PmzZ0K"
   },
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NIyK_PSzY3I",
    "outputId": "3a99345d-a201-41a6-defd-f8c9ff7340e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: eyes of bereft they in going eternal whose of end right come with to\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Generate Shakespearean-like Text\n",
    "def generate_text(model, vectorizer, seed_text, max_sequence_len, next_words=10, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = vectorizer([seed_text])\n",
    "        token_list = keras.preprocessing.sequence.pad_sequences(token_list, maxlen=max_sequence_len - 1, padding='pre')\n",
    "\n",
    "        probabilities = model.predict(token_list, verbose=0)[0]\n",
    "        probabilities = np.log(probabilities + 1e-7) / temperature\n",
    "        exp_preds = np.exp(probabilities)\n",
    "        probabilities = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        predicted_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        if predicted_index != 0:\n",
    "            output_word = vectorizer.get_vocabulary()[predicted_index]\n",
    "            seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "seed_text = \"ROMEO:\"\n",
    "print(generate_text(model, vectorizer, seed_text, max_sequence_len=max_sequence_len, next_words=20, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCMEe0ntUIb1"
   },
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWvw1L1ux16Y"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "# Use a larger subset of the corpus to improve training\n",
    "corpus = corpus[:50000]  # Limit the corpus size to 50,000 characters (then 50k)\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams from the numerical representation\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create X and y from the Padded Sequences\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "# Part 3: Define and Compile the Model using Sequential API\n",
    "\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "\n",
    "\n",
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyYO0oVe7JA-"
   },
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xF9Wlzme7JA_",
    "outputId": "3a99345d-a201-41a6-defd-f8c9ff7340e7"
   },
   "outputs": [],
   "source": [
    "# Part 5: Generate Shakespearean-like Text\n",
    "def generate_text(model, vectorizer, seed_text, max_sequence_len, next_words=10, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = vectorizer([seed_text])\n",
    "        token_list = keras.preprocessing.sequence.pad_sequences(token_list, maxlen=max_sequence_len - 1, padding='pre')\n",
    "\n",
    "        probabilities = model.predict(token_list, verbose=0)[0]\n",
    "        probabilities = np.log(probabilities + 1e-7) / temperature\n",
    "        exp_preds = np.exp(probabilities)\n",
    "        probabilities = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        predicted_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "        if predicted_index != 0:\n",
    "            output_word = vectorizer.get_vocabulary()[predicted_index]\n",
    "            seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "seed_text = \"ROMEO:\"\n",
    "print(generate_text(model, vectorizer, seed_text, max_sequence_len=max_sequence_len, next_words=20, temperature=0.8))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Xv426CS1yyQ1",
    "WR3qE1O0ONu1",
    "m9XtbhUxLFvi"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf_pro_dev] *",
   "language": "python",
   "name": "conda-env-tf_pro_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
