{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct3BN0XVSyBe"
   },
   "source": [
    "# RNN: Sequence Models\n",
    "\n",
    "In the last skill we covered the theory and concepts behind RNNs, Bidirectional RNNs, LSTMs, and GRUs.\n",
    "\n",
    "In this skill, we'll start building models to apply these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuOZmXAO5KDv"
   },
   "source": [
    "## Single-Layer RNN\n",
    "Here's the single-layer **SimpleRNN** approach: great for simpler sequence tasks where only forward processing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXdf6at65KDv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define SimpleRNN model\n",
    "\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaH9X0R01DUQ"
   },
   "source": [
    "### Bidirectional RNN\n",
    "\n",
    "Here we set up a bidirectional RNN: great for processing each input from both directions (left-to-right and right-to-left) and then combining the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzrCtIHs6Bwq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define a bidirectional RNN model\n",
    "\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfyIBBOnNGhp"
   },
   "source": [
    "\n",
    "## Single-Layer LSTM\n",
    "\n",
    "Here's a single-layer **LSTM** model: useful for processing sequence data in a single direction (left-to-right).\n",
    "\n",
    "To review: LSTMs are great at capturing long-term dependencies and designed to mitigate the vanishing gradient problem often encountered in standard RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a40e6f6"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define model parameters\n",
    "\n",
    "\n",
    "# Build single-layer LSTM model\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJZFZBn1xha9"
   },
   "source": [
    "## Bidirectional LSTM\n",
    "Here we have a bidirectional LSTM: adept at processing inputs in both forward and reverse directions and combining the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbiAKv4mxk7W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define a bidirectional LSTM model\n",
    "\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY046bfFxxPU"
   },
   "source": [
    "## Single-Layer GRU\n",
    "Here's a single-layer GRU: great for processing sequence data in a forward direction only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3cBjFZux5Si"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define a single-layer GRU model\n",
    "\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2EGpbsA6VXV"
   },
   "source": [
    "## Bidirectional GRU\n",
    "Here's a bidirectional GRU layer: when you want to processes inputs in both directions, combining the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaToI03F6d6R"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "\n",
    "# Define a bidirectional GRU model\n",
    "\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv426CS1yyQ1"
   },
   "source": [
    "# Challenge\n",
    "your task is to build a baseline model using DNN and then RNN. Aim to avoid overfitting and also consider using a functional API approach. Here is an example to review bothe **Sequential** and **Functional APIs**:\n",
    "\n",
    "### Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "GTDJtMQN7Ojy",
    "outputId": "babd3cf3-a97a-4f68-8c58-cc7182b31c09"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64\n",
    "RNN_UNITS = 16\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# SimpleRNN model: Sequential API\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM),\n",
    "    SimpleRNN(RNN_UNITS, return_sequences=True),  # Simple RNN layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name=\"Sequential_RNN_Model\")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THHO4eqt7WaY"
   },
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "7i5Y3Nj67ZWe",
    "outputId": "254b9294-f1f9-41f1-fc48-8dbf8dfe48a0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define parameters\n",
    "EMBEDDING_OUTPUT_DIM = 64\n",
    "MAX_LEN = 25\n",
    "RNN_UNITS = 16\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Define a simple RNN model using the Functional API\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "x = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_OUTPUT_DIM)(inputs)\n",
    "x = SimpleRNN(RNN_UNITS, return_sequences=True)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=inputs, outputs=outputs, name=\"Functional_RNN_Model\")\n",
    "\n",
    "# Print model summary to understand the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtA20m8d7bZD"
   },
   "source": [
    "# Challenge Starter Code\n",
    "\n",
    "### Sequrntial Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXT6adIy0Qk"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = # Limit corpus size to 10,000 chars\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences (uniform length)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Part 3: Create X and y from the Padded Sequences\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Define and Compile the Model using Sequential API\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compile model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the Model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3RlOuU6gzz7"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXT6adIy0Qk"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = # Limit corpus size to 10,000 chars\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences (uniform length)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Part 3: Create X and y from the Padded Sequences\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Define and Compile the Model using Sequential API\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compile model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the Model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3RlOuU6gzz7"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXT6adIy0Qk"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = # Limit corpus size to 10,000 chars\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences (uniform length)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Part 3: Create X and y from the Padded Sequences\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Define and Compile the Model using Sequential API\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compile model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the Model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3RlOuU6gzz7"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBXT6adIy0Qk"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Part 1: Download and Load the Corpus\n",
    "!wget https://www.gutenberg.org/files/1041/1041-0.txt -O tiny_corpus.txt\n",
    "with open('tiny_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "corpus = # Limit corpus size to 10,000 chars\n",
    "\n",
    "# Part 2: Initialize and Fit the Text Vectorization Layer\n",
    "corpus_lines = corpus.split('\\n')\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(corpus_lines).batch(1024)  # Batch to reduce memory usage\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorizer = keras.layers.TextVectorization(output_mode='int', output_sequence_length=None)\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "# Convert the entire corpus to a sequence of numbers\n",
    "sequence = vectorizer(corpus_lines)\n",
    "input_sequences = []\n",
    "\n",
    "# Generate n-grams\n",
    "for seq in sequence:\n",
    "    for i in range(1, len(seq)):\n",
    "        n_gram_sequence = seq[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence.numpy())\n",
    "\n",
    "# Pad sequences (uniform length)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Part 3: Create X and y from the Padded Sequences\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Define and Compile the Model using Sequential API\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Compile model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# Train the Model\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3RlOuU6gzz7"
   },
   "outputs": [],
   "source": [
    "# Part 4: Plot Training History\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Xv426CS1yyQ1",
    "WR3qE1O0ONu1",
    "Qd-YykQl09Ri",
    "OlhFh_V-Kzng",
    "V6yZ1dcfVl-H"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf_pro_dev] *",
   "language": "python",
   "name": "conda-env-tf_pro_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
