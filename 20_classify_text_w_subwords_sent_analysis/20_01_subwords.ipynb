{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec0005f-a2cb-452d-af66-deea317f7f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a613d0b7-972a-4103-9465-078ba7fac137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde5706f-e87d-4fef-acb4-238796a1cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd584bf-1df1-475a-b40f-df29c82a87c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40720623-561e-4bfa-9229-f345249a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e6d703-db94-4939-bc7d-6d07e1ca1dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=8185>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a23eb9-055e-4044-b14b-3abaa66a97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"On the sunny side of TensorFlow\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "orignal_text = tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b09ff3-4b1a-46d2-b8e4-733bb109c359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the sunny side of TensorFlow'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2622a2-0f4e-46a9-a07e-aad0f1a30442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[866, 1, 2365, 1361, 748, 6, 6307, 2327, 4043, 2120]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c0dee5-da15-440b-9acd-75ce0dfd0d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the sunny side of TensorFlow'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orignal_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3144f171-da02-47da-baba-5f0e8fe503e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_',\n",
       " 't_',\n",
       " 'with_',\n",
       " 'for_',\n",
       " '.<',\n",
       " 'on_',\n",
       " 'but_',\n",
       " 'movie_',\n",
       " ' (',\n",
       " 'are_',\n",
       " 'his_',\n",
       " 'have_',\n",
       " 'film_',\n",
       " 'not_',\n",
       " 'ing_',\n",
       " 'be_',\n",
       " 'ed_',\n",
       " 'you_',\n",
       " ' \"',\n",
       " 'it',\n",
       " 'd_']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.subwords[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b1ee7a-1054-47a2-924e-960c8fe9a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 866, Subword: On \n",
      "Token: 1, Subword: the \n",
      "Token: 2365, Subword: sun\n",
      "Token: 1361, Subword: ny \n",
      "Token: 748, Subword: side \n",
      "Token: 6, Subword: of \n",
      "Token: 6307, Subword: Ten\n",
      "Token: 2327, Subword: sor\n",
      "Token: 4043, Subword: Fl\n",
      "Token: 2120, Subword: ow\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Token: {token}, Subword: {tokenizer.decode([token])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b10c6e-72e9-47ce-88a1-5d6d2710c5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
